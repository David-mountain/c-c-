# 哈工大OS

## 01_认识计算机

```
一张计算机的模型图。
计算机屏幕打印出"hello"，的过程描述？
"hello" ->ascii码->汇编指令->机器指令。内存->CPU
OS：是硬件与应用软件之间的一层软件。
OS管理硬件：
	CPU管理  √
	内存管理  √
	终端管理  √
	磁盘管理  √
	文件管理  √
	网络管理
	电源管理
	多核管理
	五个√是基本部分，(一台单CPU就搞明白了，然后再去扩展网络、分布式、高级OS)
```

## 02_从开机引出计算机做了写什么？

```
计算机如何工作的？ 说白了是一个计算模型。
从图灵机开始->通用图灵机->计算机(冯诺依曼存储程序思想) 即把程序存入内存中
BIOS：基本输入输出系统。
取指执行 一条一条的指令
开始的第一条指令是固化在硬件中的，
磁盘 引导扇区读入512字节，(磁盘0扇区)
引导扇区的代码：bootsect.s(汇编代码)
512MB是什么代码呢？
汇编比C好的原因是：可以精准的控制内存。
启动盘：boot扇区  setup的4扇 system模块(OS代码)
然后到system模块。
```

## 03_OS启动

```
OS装在硬盘上的，计算机取指执行，必须将代码放在内存中，才能取指执行，所以刚开始不能OS启动。
OS -> 代码加载进内存中，才能取指执行。

bootsect.s 引导代码读入
取指执行 setup.s 
为什么叫setup ? 意思是要接管硬件了。
获取的内存大小，由创建的表管理，索引寻找。

由实模式->保护模式。
32位模式也叫保护模式，启动了32位寻址模式。
从取址->选址(表在索引中)
gdt表(全局描述符表) setup之前会初始化出来的。
setup做了哪些事？
	1. 读硬件
	2. 挪动system到00地址
	3. 启动保护模式
跳到system模块继续执行。

跳到system模块的 head.s(这是32位汇编了，不是之前的16位汇编了，后面还有个嵌入汇编) 
linux/Makefile 控制
然后是汇编跳C语言函数，本身C语言就会转成汇编的，这是其实就是汇编跳汇编。
这里会不断压栈，main函数不会退出的。
main的主要工作：内存 终中断 设备 时钟 CPU等等的初始化。

明白一个内存初始化，其他自己举一反三。
4K 页。mem_map的数组。内存前面的已经被OS使用了，后面没用的赋值0。
然后OS开始工作了。

总结：开机就做两件事。
1. 读入内存 (才能取指执行，发挥功能)
2. 初始化完成(管理硬件的软件，所以需要软件初始化，获取到硬件的参数数据)

那什么是OS接口呢？C代码+一些重要函数
OS接口，连接谁？连接OS和应用软件
如何连接？C程序。
所以，OS提供重要函数，这就是OS接口；接口表现为函数调用，又由于是系统提供，所以称为系统调用。
```

## 1.4 OS接口

```
接口：interface
自上而下说明接口。
类比插座插口，接口：连接两个东西，信号转换，屏幕细节... 
OS到底提供了什么呢？
命令是？一个用C语言写的程序而已，shell也是一段程序。
命令里增加了一些重要的函数，对硬件的使用：
	printf 图形
	scanf 键盘
	exec cup申请
硬件输入 -> 系统消息队列 -> (每取一个消息，触发对应函数)消息循环

系统调用，像电源插头一样，先从认识插头开始，便是OS常识。
POSIX:IEEE执行的标准簇。 X: UNIX
分类		POSIX定义     描述
任务管理   fork 		创建一个进程
		  exed		   运行一个 可执行进程
		  pthread_create 创建一个线程
文件系统   open 		打开一个文件或目录
		  EACCES	   返回值，表示无权限s
		  mode_t st_mode 文件头结构 文件属性
```

## 1.5 系统调用的实现 (system call)

```
插座背后的电路是如何实现的呢？
数据都在内存中，为啥不能直接获取呢？
直接跳过去获取，变成了函数调用，不是系统调用了。
若那样可随意调用数据,jump，那数据不安全了，通过应用程序就可以看到内存中其他的数据了。

系统调用，进入内核的门。
内核(用户)态  内核(用户)段-硬件 (cs对内存利用是一段一段划分的)
将内核程序和用户程序隔离。
区分内核态，用户态:一种处理器，硬件设计。
当前程序执行在什么态(哪一层)?
由于CS:IP是当前指令，所以用CS的最低两位来表示。0是内核态，3是用户态

OS和硬件紧密相连：
内核态可访问any data
用户态不可访问内核data 
每次访问都要判断 所处什么级别。

当然，硬件提供了主动进入内核的方法，即中断。
X86，即终端指令int 
int指令将使CS的CPL改成0，进入内核
这是用户程序发起的调用内核代码的唯一方式。

系统调用的核心：
1. 用户程序包含一段 int指令的代码
2. OS写中断处理，获取想调程序的编号
3. OS根据编号执行相应代码
这就是插座背后的故事。

int 0x80中断的处理：
去IDT表取出中断处理函数，跳去执行，处理完之后再回来
中断：暂停，干另一件事，再回来继续下面的事
idt表：全局变量的表。

printf用户调用 -> printf int 0x80    --用户态
-> 中断处理 system call -> 查表call_table -> NR_write -> 调用sys_write  --内核态 
```

## 1.6 OS历史 

```
读史明智
单道程序 批处理系统 
多道程序 多进程(切换和调度)
分时系统(核心是任务切换)

OS理论学习只有10%，实践是90%。
多进程图谱：CPU 内存 这两个懂了，就差不多了。

文件操作视图。 I/O就清楚了，磁盘/文件
多进程 + 文件操作视图 搞定，OS在你脑中就清楚了。
```

## 1.7 

```
后面几十课都围绕：多进程图谱 + 文件操作视图
cpu 内存  --OS如何管理呢？多进程
显示器 键盘/打印机 磁盘 --文件操作视图
```

## 1.8 CPU管理的直观想法

```
你能自己提出问题，并且能自己寻找办法解决问题！！！就很厉害了。

OS管理CPU，就是管理CPU，才引出的多进程图像
管理CPU，先要会使用CPU：
一个程序放入内存，然后发出一个地址，如50，地址总线上放50
管理CPU，设好PC初值就可以了，它自动取指执行

cpu快I/O很多，前者是电子设备，后者是机械设备
某种情况下，若CPU一直等待I/O，利用率太低了，咋办？
等待期间切换到其他程序执行，即多道程序，交替执行
CPU是CS核心，CPU工作了，其他设备也被带动工作了，利用率就上去了

一个CPU面对多个程序？
一个CPU上交替的执行多个程序：并(同时)发(触发)
如何做到呢？
修PC寄存器就行了吗？
切到另一个程序，要记住地址，因为可能会切回来的
这就需要一个数据结构记录信息：PCB，切回来的时候需要保持原样哈

运行的程序和静态程序不一样，需要描述这些不一样，传统有没有相关的概念，创造一个概念，引进了进程，即进行中的程序
进程有开始、有结束、程序没有
进程走走停停，走停对程序没有意义
进程需要记录ax bx... 程序不用
所有的不一样都装在PCB中
这就慢慢形成了多进程的概念 --多个进程
```

## 1.9 多进程图像

```&gt;
这节课宏观说多进程图像
启动了的程序就是进程，多个进程推进
合理次序推进，即多进程图像

fork()启动第1个进程  main中的fork()创建了第一个进程，init执行了shell(windows则是桌面)
shell再启动其他进程 每要解决一个问题，启动1一个进程 

命令启动一个进程，返回shell再启动其他进程。
任务管理器可以看到，启动了哪些进程。explorer-dos 
当哪个进程CPU利用率最高，影响到其他进程使用了，可以直接关闭这个进程

用户管理使用CS，就是管理启动了一堆进程

多进程如何组织呢？
OS感知进程全靠PCB，(记录进程信息的数据结构)
OS组织进程，也靠PCB

一个进程正在执行，其他等着，形成了队列数据结构了
每个进程的PCB放在哪里，OS都知道的，到谁就执行谁

为了更好的管理：多进程的组织  PCB+状态+队列
新建态 -> 就绪态 <-> 运行态 -> 终止态
		      阻塞态 
		      
这样好理解，管理

多进程如何交替？灵魂，即切换
schedule()函数，这个明白了，切换就明白了，
{
  pNew = getNext(Ready Queue); // 调度，到底选哪个进程合适呢？
  switch_to(pCur, pNew); // 切换 pCur pNew都是PCB
}

若要形成多进程图像？
读写PCB，OS最重要的结构，贯穿始终
操作寄存器完成切换 10 11 12 
写调度程序 13  14 
有进程同步与合作 16 17 
要有地址映射 20 
```

## 1.10 用户级线程

```
不要凭空想象，学习编程要实际代码跑起来，就懂了

一个进程执行一堆指令
不切换映射表(切换到另外一个进程)，就可以切换指令不？ 引出了线程概念
进程 = 资源 + 指令执行序列
将资源和指令执行分开，一个资源+多个指令的指令序列

内存不用切了，快很多，既保留了并发特点，又避免了进程切换的代码

学内存时，再学映射表切换(内存切换)
这里说指令的切换 (分而治之的思想，搭积木，最后组合)
实质是：映射表不变 而PC指针变

举例了一个网页浏览器加载过程的变化：省略
多进程的切换只比 多进程的切换少了一步(不涉及资源切换)

线程切换
用户级线程，使用要主动调用
create(创建）Yield(交替）这两个函数搞清楚即可
Yield(核心)：能切换 就知道切换时 需要是个什么样子 
create:制造出第一次 应该的样子

2个执行序列与一个栈
有问题？
不仅Yield跑来跑去了待在其他进程了

函数调用应该有自己的栈，栈拆分成2个栈，Yield切换前先要切栈
栈的东西要存放起来，引出了TCB(全局结构)(CPU的寄存器)

即：栈 + TCB的配合 

两个线程的样子：2个TCB 2个栈 切换的PC在栈中

ThreadCreate核心就是用程序作出这三个东西
void ThreadCreate (A)
{
  TCB *tcb = malloc();
  *stack = A;
  *stack = A;
  tdb.esp = stack;
}

内核不知道你线程，内核这只有进程的概念，举例：浏览器一个标签卡了，别的标签也打不开了，切到别的进程去了
若进程 某个线程进入内核并阻塞了
ThreadCreate是系统调用，会进入内核，内核知道TCB，内核级线程并发性更好

内核叫schedule(调度方式) 是系统调用
Yield()用户不管，调度点由OS决定

```

## 1.19 信号量

```c

```



## 1.20 内存使用与分段_memory and segmentation

```c
这节开始学习管理内存
学习思路：学习内存，首先要让内存用起来，然后引出问题，再具体探讨
要让内存用起来，先要让计算机工作起来
冯诺依曼存储程序思想，把程序加载进内存，计算机工作了，内存不就被带动工作了嘛 

引出问题，如何让程序进入内存呢？
地址总线 数据总线 控制总线
找到地址40处，现在是程序必须放入这个物理地址，否则无法执行
从常理说，应该是找个空闲的内存，把程序放入，而且从地址开始处，放的是OS代码

所以需要地址空定位? 这里说运行时重定位，不说编译时，不说载入时
相对地址，也叫逻辑地址
什么时候完成重定位?
两种：1. 编译时 2.载入时(灵活点)
  有的系统硬，就是这个地址只能放这个代码，固定位置，但是PC多不是，哪里空闲就放，加对应基址即可
  
程序载入后，还需要移动？ 交换swap
进程堵塞了，OS就会让他睡眠，放入磁盘，加载其他的进程，下次加载进入内存，地址就会发生变化的，需要重定位
即：每执行一条指令都要从逻辑地址算出物理地址，地址翻译
加上基地址，base+300(offset)，这些放在PCB中，每个进程都有各自的基地址，放在PCB中

------
引入分段：OS是将整个内存，一起载入内存吗？
怎么定位具体指令(数据)呢？ <段号， 段内偏移> mov [es: bx], ax 
分段(分治思想)，符合用户，可独立考虑每个段，方便管理
每个段有各自的特点，代码段只读，代码/数据段不会动态增加
主程序(只读) 变量集 函数库 动态数组 栈 

将各段分别放入内存
前面定位指令是 <段号，段内偏移>
重定位(也是加了段的基址)，不同段加不同基址

    段号    基址   长度   保护
进   0     180k    150k   R
程   1     360k    60k   R/W
段   2     70k     110k  R/W
表   3     460k    40k    R

之前是GDT表，这是GDT表粗略的样子
OS的信息-GDT表-OS对应的段-global
其他进程-LDT表 
```



## 1.21 内存分区与分页

```c
memory partition and paging 

回顾：
程序分段处理，不同段有不同的特点(分治的思想)，方便管理
运行时重定位，哪个段放到内存哪个地方，需要记住这些信息，将来好重定位--LDT表，即对应的寄存器
这便是一个进程，一个进程一个LDT表
进程切换，LDT表也跟着切换
实验六可以做了

内存中如何找出一段空闲的分区？
把程序加载进来，但是要讲到后面的文件系统和磁盘管理才知道如何读取
1. 分段 
2. 找空闲(一个算法)
3. 将映射关系做好
(这几个步骤几乎把CS大部分学科的知识都涵盖了，所以说OS是一个系统)
  
找内存，内存如何分割？
1. 固定分区
  等分，OS初始化时，将内存等分成K个分区，但孩子有大有小，段也有大有小，需求不一定
2. 可变分区
  可变分区的管理过程--核心是数据结构
  空闲分区表、已分配分区表
  然后来了一个请求，段内存请求：reqSize: 100k
  然后去空闲分区表查看，如何满足则返回，不满足情况暂时不考虑
  然后更新已分配分区表 
  释放内存呢？也是会更新这两个表
  
  空闲分区表：
  	始址   长度   
  	350K  150K
  	200K  50K
  已分配分区表：
  	始址   长度   标志
  	0K     100K   OS
  	100K   100K   seg1
  	200k   500k   seg2
  	
  又有 一个段提出内存请求？reqSize: 40K
  左边2个空间都可以，这时候应该如何选择呢？
  算法：无对错，只有优劣，综合考虑
  	首先适配 (350, 150)
  	最佳适配 (200, 50)
  	最差适配 (350, 150)
  (50K离它最近，最佳)
   这样的话，最后空闲区间会越来越小，若选最大的，则最后均匀区间
   
引入分页：
  解决内存分区导致的内存效率问题，物理内存是用分页来割的，中间还有个虚拟内存，
  分区的实际是对虚拟内存的处理
  物理内存(分页) -- 虚拟内存(分区)
  可变分区造成的问题：
    发起请求reqSize: 160咋办？
    但是没有一个空闲分区>160 (段是连续的前提哈)
    这是内存碎片，多用分页就没有这个问题
    
   将空闲分区合并，需要移动1个段(复制内容):内存紧缩
   移动过程中，不能执行，死机状态，CPU上层服务，因为地址在变
   内存紧缩需要花大量实际，如果复制速度1M/1秒，则1G内存的紧缩时间为1000S~=17min 
   为啥不能将请求拆分：160 = 120 + 40，从连续到离散，将内存分成页 
   针对每个段内存请求，系统一页一页的分配给这个段
   问题：此时需内存紧缩吗？
   最大内存浪费是多少？一页(一个段)4K
   一个进程最多浪费4K
   物理内存用分页，用户角度希望分段，程序多个段组成
   OS两个都应支持，合在一起，内存管理轮廓就有了
   
   jmp 40 若一个页单位是100，应跳到多少页？
   	页框5(对应物理地址500)：段0：页0
   	即：500 + 40 = 540 
   页放在页框5中，页中的地址就需要重定位
   页中的仍然是逻辑地址：
   		page#(第几页)   |   offset(页面尺寸 4K)
  mov[0x2240], %eax 
  逻辑地址：0x02 0x240
      页号   页框号  保护
      0       5      R
      1       1      R/W
      2       3      R/W
      3       6      R
      (页表指针 PCB中应该有这些)
```

## 1.22 多级页表与快表

```c
分页知识实际有问题
上集分页机制 + 多级页表 + 快表 ~= 较高级的机制
为提高内存空间利用率，页
页号 页框号 保护
0	5		R
1	1		R/W
2	2		R/W
3	6		R

页号是逻辑页号，页小了，逻辑页号多了
与页框建立对应关系
页框号也多了，但这样页表项太多了，视频中计算100W多个，太多了
页表会很大，页表放置就成了问题：
  页面尺寸通常4K，而地址32位的，有2^20个页面
  2^20个页表都得放到内存中，需4M内存
  系统中并发10个进程，就需要40M内存
  更多进程呢？内存消耗更多，内存浪费
  实际上大部分逻辑地址根本不会用到！！！
  
第一次尝试：只存放用到的页：
  但这样页表中的页号不连续，需比较 查找算法
  二分查找 log
  页表功能：根据逻辑页号可找到页框号(物理上的)
  
  不行，不连续消耗性能了，即使log只有10次，那本身1次的，这就多了9次
  改成顺序的，页面连续(一个偏移量即达到)
  32位地址空间+4K页面+页面必连续 ->2^20个页表项 -> 大页表占用内存  造成浪费
  如何解决？
  既要连续，又要内存占用少？
  提出方案：多级页表
  类似书目录，章节目录，从生活中抽象
  
  章目录 节目录
  第一章 1.1 1.2..
  第二章
  ...
  (连续号 只比几次)
  
  第二种尝试：多级页表，即页目录表(章) + 页表(节)
  页目录号 页号 offset 
  10bits  10bits 12bits
  10bits = 2^10
  2^10 * 4K = 4M
  每个页目录下，4M页号空间
  
  多级页表提高了空间效率，那时间上呢？
  增加了访问的次数，尤其是64位系统
  TLB是一组相联快速存储的寄存器
  如何解决呢？
  快表，TLB
  可以找到最近使用的逻辑页对应的物理页
  页号(逻辑地址) offset -> (物理地址)物理页号 offset -> 命中 
  TLB未命中失败 - 多级页表
  
  体现了OS折中的思想
  如何让TLB更加高效，提高命中率？
  有效访问时间 = HitR * (TCB + MA) + (1-HitR) * (TLB+2MA)
  			  命中率     内存访问时间            TLB时间
  			 = 98%(20ns+100ns(保守估计)) + 2%*(20ns+200ns) = 122ns
  要想真正实现近似访问1次，提高命中率
  TLB越大越好，但TLB贵，寄存器
  通常只有[64, 1024]
  为什么是这个范围呢？
  相比2^20个页，64很小，为啥TLB能起作用？
  程序的地址访问存在局部性(程序多体现为循环、顺序结构)
  空间局部性(也反映快表的合理性)
```

## 1.28 生磁盘的使用

```c
设备管理的最后一个管理
磁盘管理(OS最后一块内容)
磁盘管理的原理和之前一样，但是比之前的I/O设备要复杂很多，因为经过更多的包装层次 (后面4讲都是说这个)

先让磁盘转起来(生磁盘) -> 更好的转起来 -> 抽象，引出文件(熟磁盘)

  认识磁盘：
  磁盘的访问单位是扇区 
  扇区大小是512字节
  扇区大小是传输时间和碎片浪费的折中
  
  磁盘I/O过程：
  控制器->寻道->旋转->传输
  (磁生电 磁信号->电信号，读出一个字节    电信号->磁信号，写入一个字节)
  1. 移动磁头，到那个磁道上
  2. 转磁道，移动道那个扇区
  3. 一边旋转，一遍磁生电，电生磁
  和内存缓冲区进行数据交互，是读还是写
  
  使用磁盘：
  只要往控制器中写 柱面(C) 磁头(H) 扇区(S) 缓存位置
  用什么写呢？还是out指令
  
  out指令，就可以使用磁盘，但是太麻烦了，需要向上封装，抽象，用户使用不管细节，最后一层-文件系统
  通过盘块号读写磁盘(一层抽象)
  抽象->block->磁盘驱动->S H C->磁盘控制器->
  磁盘驱动负责从block(一个维度)计算出CHS(三个维度，柱面，立体)
  问题是如何编址呢？为何这样编？
  
  只要发个块号，即可，方便了。
  如何更加高效呢？block相邻的盘块可快速读出
  磁盘访问时间 = 写入控制器时间 + 寻道时间(12ms to 8ms)+ 旋转时间(7200转/min) + 传输时间(50M/S 约0.3ms)
  其中寻道时间最长，机械运动比电要慢，优化
  省略。需要图方便解释。
  
  从CHS到扇区号，从扇区到量块
  问：CHS得到的扇区号是？
  C * (Heads * sectors) + H * Sectors + S = block
       柱面几个  一层磁头几个
  S = block % sectors
  
  从扇区到盘块，忽略传输时间，太小
  每次读写1K，碎片0.5K; 读写速度100k/s
  每次读写1M，碎片0.5M; 读写速度40M/s
  
  可利用空间很严重，换时间，因为现在硬盘便宜 
  
  现在单位从 扇区到盘块 发个盘块号即可
  问：Linux0.11盘块有多大？
  第一个盘块号  对应扇区
  0 -> 0
   1-> 2
    2 -> 4
    2个扇区。
    
多个进程通过排队队列使用磁盘(第二层抽象)
    多个磁盘访问请求，咋办？
    磁盘调度
    调度的目标是什么？调度时主考察什么？
    目标是平均访问延迟，寻道时间是主要矛盾
    给调度算法，仍然从FCFS开始
    
   FCFS磁盘调度算法：
   一个实例：磁头开始位置 = 53
    请求队列 = 98，183，37.。。
    FCFS：磁头共移动640磁道
    磁头长途奔袭，为啥不在移动过程中把经过的请求处理了，这样
    磁头共移动了236磁道，少很多了
    
    C-SCAN（电梯算法):
	scan+直接移到另一端，两端请求都是能很快处理的
	
	raw dist 生磁盘完整流程？
	1. 进程 得到盘块号？后面说 算出扇区号sector
	2. 用扇区号make req,用电梯算法add_request
	3. 进程sleep_on (进程放到请求队列，任务就完成了)
    4. 磁盘中断处理
    5. do_hd_request算出CHS
    6. hd_out调用outp(...) 完成端口写 (唤醒进程哈)
```

